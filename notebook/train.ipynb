{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# CLIP for feature extraction\n",
        "import clip\n",
        "\n",
        "\n",
        "# --- Step 1: Dataset unzip & prep ---\n",
        "ZIP_PATH = \"images_compressed.zip\"\n",
        "IMG_DIR = \"images\"\n",
        "CSV_PATH = \"images.csv\"\n",
        "\n",
        "# Unzip images if not already\n",
        "if not os.path.exists(IMG_DIR):\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(IMG_DIR)\n",
        "    print(\"âœ… Images extracted to:\", IMG_DIR)\n",
        "else:\n",
        "    print(\"âœ… Images already extracted:\", IMG_DIR)\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# Add full image paths (assuming .jpg extension, change if .png)\n",
        "df['image_path'] = df['image'].apply(lambda x: os.path.join(IMG_DIR, x + \".jpg\"))\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0QprP11Pj5k",
        "outputId": "21eacbb3-7632-4ef3-85a2-160291635d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Images already extracted: images\n",
            "Dataset shape: (5403, 5)\n",
            "Columns: Index(['image', 'sender_id', 'label', 'kids', 'image_path'], dtype='object')\n",
            "                                  image  sender_id     label   kids  \\\n",
            "0  4285fab0-751a-4b74-8e9b-43af05deee22        124  Not sure  False   \n",
            "1  ea7b6656-3f84-4eb3-9099-23e623fc1018        148   T-Shirt  False   \n",
            "2  00627a3f-0477-401c-95eb-92642cbe078d         94  Not sure  False   \n",
            "3  ea2ffd4d-9b25-4ca8-9dc2-bd27f1cc59fa         43   T-Shirt  False   \n",
            "4  3b86d877-2b9e-4c8b-a6a2-1d87513309d0        189     Shoes  False   \n",
            "\n",
            "                                        image_path  \n",
            "0  images/4285fab0-751a-4b74-8e9b-43af05deee22.jpg  \n",
            "1  images/ea7b6656-3f84-4eb3-9099-23e623fc1018.jpg  \n",
            "2  images/00627a3f-0477-401c-95eb-92642cbe078d.jpg  \n",
            "3  images/ea2ffd4d-9b25-4ca8-9dc2-bd27f1cc59fa.jpg  \n",
            "4  images/3b86d877-2b9e-4c8b-a6a2-1d87513309d0.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCcKpyMltj2W",
        "outputId": "3c001661-0c79-4175-9c54-ae78ba198e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['image', 'sender_id', 'label', 'kids', 'image_path'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from PIL import Image\n",
        "\n",
        "def safe_open_image(path):\n",
        "    try:\n",
        "        return Image.open(path).convert(\"RGB\")\n",
        "    except:\n",
        "        # fallback black image\n",
        "        return Image.new(\"RGB\", (224,224), (0,0,0))\n",
        "\n",
        "# --- Step 2: Dataset class ---\n",
        "class WardrobeDataset(Dataset):\n",
        "    def __init__(self, df, transform=None, use_metadata=True):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.use_metadata = use_metadata\n",
        "        self.classes = sorted(df['label'].unique())\n",
        "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image = safe_open_image(row['image_path'])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.class_to_idx[row['label']]\n",
        "\n",
        "        # Metadata (only kids flag here, can extend later)\n",
        "        metadata = torch.tensor([int(row['kids'])], dtype=torch.float32) if self.use_metadata else torch.zeros(1)\n",
        "\n",
        "        return image, label, metadata\n",
        "\n"
      ],
      "metadata": {
        "id": "M-AuZGvDQ3Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3: Transforms ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                         std=[0.26862954, 0.26130258, 0.27577711])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                         std=[0.26862954, 0.26130258, 0.27577711])\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "quysHUZfQ7cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: CLIP Feature Extractor ---\n",
        "class CLIPFeatureExtractor(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super().__init__()\n",
        "        self.model, _ = clip.load(\"ViT-B/32\", device=device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def forward(self, images):\n",
        "        with torch.no_grad():\n",
        "            features = self.model.encode_image(images)\n",
        "        return features\n"
      ],
      "metadata": {
        "id": "FHsJaNHWQ7Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "embeddings = []\n",
        "img_ids = []\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    img_path = f\"/content/images/{row['image']}.jpg\"  # adjust extension if png\n",
        "    try:\n",
        "        image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            emb = model.encode_image(image).cpu().numpy().flatten()\n",
        "        embeddings.append(emb)\n",
        "        img_ids.append(row['image'])\n",
        "    except:\n",
        "        print(\"Failed:\", img_path)\n",
        "\n",
        "embeddings = np.array(embeddings)\n",
        "np.save(\"clip_embs.npy\", embeddings)\n",
        "\n",
        "# Save ids alongside\n",
        "np.save(\"img_ids.npy\", np.array(img_ids))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X48C4-aIhr_Q",
        "outputId": "008bfeb4-085c-4a4a-96a1-b9b13f0ade4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|â–‰         | 533/5403 [02:05<27:08,  2.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed: /content/images/d028580f-9a98-4fb5-a6c9-5dc362ad3f09.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|â–ˆâ–Ž        | 703/5403 [02:44<22:31,  3.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed: /content/images/1d0129a1-f29a-4a3f-b103-f651176183eb.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|â–ˆâ–Œ        | 861/5403 [03:21<15:42,  4.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed: /content/images/784d67d4-b95e-4abb-baf7-8024f18dc3c8.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|â–ˆâ–ˆâ–ˆ       | 1662/5403 [06:28<14:04,  4.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed: /content/images/c60e486d-10ed-4f64-abab-5bb698c736dd.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1763/5403 [06:51<12:53,  4.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed: /content/images/040d73b7-21b5-4cf2-84fc-e1a80231b202.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5403/5403 [21:04<00:00,  4.27it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 5: Compatibility Model ---\n",
        "class OutfitCompatibilityNet(nn.Module):\n",
        "    def __init__(self, embed_dim, metadata_dim=1):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(embed_dim * 2 + metadata_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, top_embed, bottom_embed, metadata=None):\n",
        "        if metadata is None:\n",
        "            metadata = torch.zeros(top_embed.size(0), 1, device=top_embed.device)\n",
        "        x = torch.cat([top_embed, bottom_embed, metadata], dim=1)\n",
        "        return self.fc(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "eZ8XNct8Q7Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 6: Color Harmony Explainability ---\n",
        "def extract_dominant_colors(img_path, k=3):\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img_small = img.resize((64, 64))\n",
        "    arr = np.array(img_small).reshape(-1, 3)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
        "    kmeans.fit(arr)\n",
        "    colors = kmeans.cluster_centers_.astype(int)\n",
        "    return colors\n",
        "\n",
        "\n",
        "def explain_color_match(img1_path, img2_path):\n",
        "    colors1 = extract_dominant_colors(img1_path, k=3)\n",
        "    colors2 = extract_dominant_colors(img2_path, k=3)\n",
        "\n",
        "    for c1 in colors1:\n",
        "        for c2 in colors2:\n",
        "            if np.allclose(c1 + c2, [255, 255, 255], atol=50):\n",
        "                return f\"Complementary match: {c1} vs {c2}\"\n",
        "    return \"Analogous or neutral harmony detected\"\n",
        "\n"
      ],
      "metadata": {
        "id": "_TNNMeAiRHZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 7: Training Loop ---\n",
        "def train_model(clip_extractor, model, train_loader, criterion, optimizer, device, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for images, labels, metadata in train_loader:\n",
        "            images, labels, metadata = images.to(device), labels.to(device), metadata.to(device)\n",
        "\n",
        "            embeddings = clip_extractor(images)\n",
        "\n",
        "            # Pair adjacent samples in batch\n",
        "            top_embed = embeddings[::2]\n",
        "            bottom_embed = embeddings[1::2]\n",
        "            meta = metadata[::2]\n",
        "\n",
        "            labels_pair = (labels[::2] == labels[1::2]).float().unsqueeze(1)\n",
        "\n",
        "            preds = model(top_embed, bottom_embed, meta)\n",
        "            loss = criterion(preds, labels_pair)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4QnAdKLiRHXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 8: Main ---\n",
        "if __name__ == \"__main__\":\n",
        "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "\n",
        "    train_dataset = WardrobeDataset(train_df, transform=train_transform)\n",
        "    val_dataset = WardrobeDataset(val_df, transform=test_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    clip_extractor = CLIPFeatureExtractor(device)\n",
        "    model = OutfitCompatibilityNet(embed_dim=512, metadata_dim=1).to(device)\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Train\n",
        "    train_model(clip_extractor, model, train_loader, criterion, optimizer, device, epochs=5)\n",
        "\n",
        "    # Demo Explanation\n",
        "    img1 = df.iloc[0]['image_path']\n",
        "    img2 = df.iloc[1]['image_path']\n",
        "    print(\"Color Explanation:\", explain_color_match(img1, img2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idL4wcABRHU7",
        "outputId": "d59d87ea-8172-4921-bbbe-3e0f6d85578d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 46.0534\n",
            "Epoch 2/5, Loss: 43.2458\n",
            "Epoch 3/5, Loss: 41.3490\n",
            "Epoch 4/5, Loss: 41.5357\n",
            "Epoch 5/5, Loss: 34.4408\n",
            "Color Explanation: Complementary match: [179 183 152] vs [119  26  57]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/model_weights.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Mh0QyD3RHSl",
        "outputId": "4861558b-5ba2-4b4a-9b68-5d8c2a0f9866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"compatibility_model.pth\")\n"
      ],
      "metadata": {
        "id": "kS7AJi_APnsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "\n",
        "# Load the model weights\n",
        "model = OutfitCompatibilityNet(embed_dim=512, metadata_dim=1).to(device)\n",
        "model.load_state_dict(torch.load(\"compatibility_model.pth\", map_location=device))  # saved after training\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# --- Prediction function ---\n",
        "def get_compatibility(img1_id, img2_id):\n",
        "    row1 = df[df['image'] == img1_id].iloc[0]\n",
        "    row2 = df[df['image'] == img2_id].iloc[0]\n",
        "\n",
        "    img1 = Image.open(row1['image_path']).convert(\"RGB\")\n",
        "    img2 = Image.open(row2['image_path']).convert(\"RGB\")\n",
        "\n",
        "    img1_t = test_transform(img1).unsqueeze(0).to(device)\n",
        "    img2_t = test_transform(img2).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        emb1 = clip_extractor(img1_t)\n",
        "        emb2 = clip_extractor(img2_t)\n",
        "\n",
        "        metadata = torch.tensor([[int(row1['kids'])]], dtype=torch.float32).to(device)\n",
        "        score = model(emb1, emb2, metadata)\n",
        "\n",
        "    explanation = explain_color_match(row1['image_path'], row2['image_path'])\n",
        "    return score.item(), explanation\n",
        "\n",
        "\n",
        "# --- Main CLI ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if running in Colab\n",
        "    if 'google.colab' in str(get_ipython()):\n",
        "        print(\"Running in Google Colab. Using example image IDs.\")\n",
        "        # Provide example image IDs from your dataframe\n",
        "        img1_id_example = df.iloc[0]['image']\n",
        "        img2_id_example = df.iloc[1]['image']\n",
        "        score, explanation = get_compatibility(img1_id_example, img2_id_example)\n",
        "        print(f\"\\nðŸ‘— Compatibility between {img1_id_example} and {img2_id_example}: {score:.2f}\")\n",
        "        print(\"ðŸŽ¨ Explanation:\", explanation)\n",
        "    else:\n",
        "        parser = argparse.ArgumentParser(description=\"Outfit Compatibility Inference\")\n",
        "        parser.add_argument(\"--img1\", type=str, required=True, help=\"Image ID of first item\")\n",
        "        parser.add_argument(\"--img2\", type=str, required=True, help=\"Image ID of second item\")\n",
        "        args = parser.parse_args()\n",
        "\n",
        "        score, explanation = get_compatibility(args.img1, args.arg2)\n",
        "        print(f\"\\nðŸ‘— Compatibility between {args.img1} and {args.img2}: {score:.2f}\")\n",
        "        print(\"ðŸŽ¨ Explanation:\", explanation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KfvWZXLQ7VS",
        "outputId": "f61a143d-991d-40fd-f750-9caff4fb6ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab. Using example image IDs.\n",
            "\n",
            "ðŸ‘— Compatibility between 4285fab0-751a-4b74-8e9b-43af05deee22 and ea7b6656-3f84-4eb3-9099-23e623fc1018: 0.52\n",
            "ðŸŽ¨ Explanation: Complementary match: [179 183 152] vs [119  26  57]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "edp1XPngx0-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch\n",
        "import clip\n",
        "\n",
        "# Paths\n",
        "CSV_PATH = 'images.csv'\n",
        "IMG_DIR = 'images'\n",
        "#EMB_PATH = 'clip_embs.npy'\n",
        "import numpy as np\n",
        "\n",
        "EMB_PATH = \"clip_embs.npy\"\n",
        "ID_PATH = \"img_ids.npy\"\n",
        "\n",
        "embs = np.load(EMB_PATH)\n",
        "img_ids = np.load(ID_PATH)\n",
        "\n",
        "\n",
        "# Load dataset and embeddings\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "df['image_path'] = df['image'].apply(lambda x: f'{IMG_DIR}/{x}.jpg')\n",
        "embs = np.load(EMB_PATH)\n",
        "\n",
        "# Setup CLIP\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, preprocess = clip.load('ViT-B/32', device=device)\n",
        "model.eval()\n",
        "\n",
        "# Build Nearest Neighbors\n",
        "nn = NearestNeighbors(n_neighbors=100, metric='cosine').fit(embs)\n",
        "\n",
        "import os\n",
        "\n",
        "def recommend_full_outfit_ui(base_img_path, topk=5):\n",
        "    print(\"Base image received:\", base_img_path)\n",
        "    img_filename = os.path.basename(base_img_path)\n",
        "    query_str = os.path.join('images', img_filename)\n",
        "\n",
        "    # Find matching row using the correct column and variable names\n",
        "    baserow = df[df['image_path'] == query_str]\n",
        "    if len(baserow) == 0:\n",
        "        print(f\"Uploaded image {img_filename} not found in dataset.\")\n",
        "        return []\n",
        "\n",
        "    baseidx = baserow.index[0]\n",
        "    basevec = embs[baseidx].reshape(1, -1)\n",
        "\n",
        "    dists, inds = nn.kneighbors(basevec, n_neighbors=topk+1)\n",
        "\n",
        "    recs = []\n",
        "    for i in inds[0]:\n",
        "        if i == baseidx:\n",
        "            continue\n",
        "        img_path = df.iloc[i]['image_path']\n",
        "        recs.append((img_path, \"\"))  # tuple with (image_path, caption)\n",
        "\n",
        "    return recs\n",
        "\n",
        "\n",
        "# Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=recommend_full_outfit_ui,\n",
        "    inputs=gr.Image(type='filepath', label='Upload base item'),\n",
        "    outputs=gr.Gallery(label='Recommended Outfit Items'),\n",
        "    title='Smart Outfit Recommender',\n",
        "    description='Upload a clothing item to get full outfit recommendations.'\n",
        ")\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-crJTmRReyjU",
        "outputId": "7cf6d24c-7f08-46ec-8771-0a1b8d033e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQHwhOOrfw-v",
        "outputId": "c2006639-717d-40ae-aba0-8bee7cf4f68f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://2ab6ac24e0bf1789be.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Base image received: /tmp/gradio/9a3134816dd28397a346c644ae1ded989e62c727f5dcdbcbc3a2d5b4ee65442d/0b55a8e8-0087-4c19-8729-b872718ff5ae.jpg\n",
            "Base image received: /tmp/gradio/9a3134816dd28397a346c644ae1ded989e62c727f5dcdbcbc3a2d5b4ee65442d/0b55a8e8-0087-4c19-8729-b872718ff5ae.jpg\n",
            "Base image received: /tmp/gradio/6bde9614f79f4a73eac35dcf3f9fb6a47711717ea415da92ec587fc92a899f4d/0b7f4987-34e4-4c85-9f28-35e04ae78ece.jpg\n",
            "Base image received: /tmp/gradio/6bde9614f79f4a73eac35dcf3f9fb6a47711717ea415da92ec587fc92a899f4d/0b7f4987-34e4-4c85-9f28-35e04ae78ece.jpg\n",
            "Base image received: /tmp/gradio/fc5b120cc50cd70eb5dc0a4136b06db427a4c18a6dbb8c3500da5c36d6738a1e/0a2668d3-e42a-4f46-bb7f-01cc409c1839.jpg\n",
            "Base image received: /tmp/gradio/6bde9614f79f4a73eac35dcf3f9fb6a47711717ea415da92ec587fc92a899f4d/0b7f4987-34e4-4c85-9f28-35e04ae78ece.jpg\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3158, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/app.py\", line 72, in <module>\n",
            "    demo.launch(share=True)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3055, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3160, in block_thread\n",
            "    print(\"Keyboard interruption in main thread... closing server.\")\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://2ab6ac24e0bf1789be.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cGn1SZCogftn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}